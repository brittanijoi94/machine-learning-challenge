{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=94e59fcefe49e75ac0e080e642a94ab5c25a727db1b48812e38b0daa21d3c954\n",
      "  Stored in directory: /Users/brittaniterry/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12180, 86)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4702, 86)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12180, 86)\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data\n",
    "# Get X data by removing loan_status column (outcome)\n",
    "X = train_df.drop(['loan_status', 'Unnamed: 0'], axis=1)\n",
    "X_dummies = pd.get_dummies(X, drop_first=True)\n",
    "print(X_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4702, 85)\n",
      "debt_settlement_flag_Y\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "X_test = test_df.drop(['loan_status', 'Unnamed: 0'], axis=1)\n",
    "X_test_dummies = pd.get_dummies(X_test, drop_first=True)\n",
    "print(X_test_dummies.shape)\n",
    "\n",
    "for el in X_dummies.columns:\n",
    "    if el not in X_test_dummies.columns:\n",
    "         print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing dummy variables to testing set\n",
    "X_test_dummies['debt_settlement_flag_Y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output labels to 0 and 1\n",
    "y_label = LabelEncoder().fit_transform(train_df['loan_status'])\n",
    "y_test_label = LabelEncoder().fit_transform(test_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the unscaled data and print the model score\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_dummies, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\\\"Training Data Score: {logmodel.score(X_dummies, y_label)}\\\")\n",
    "# print(f\\\"Testing Data Score: {logmodel.score(X_test_dummies, y_test_label)}\\\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6707783921735432\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=50).fit(X_dummies, y_label)\n",
    "print(f'Training Score: {clf.score(X_dummies, y_label)}')\n",
    "print(f'Testing Score: {clf.score(X_test_dummies, y_test_label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that the after scaling the data to remove bias from features with higher magnitudes, the Random Forest Classifier will perform better than the Logistic Regression model both in the training set and in the testing set, since the Random Forest model is designed for more complex relationships between features and doesn't assume linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_dummies)\n",
    "X_train_scaled = scaler.transform(X_dummies)\n",
    "X_test_scaled = scaler.transform(X_test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6714164185452999\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "clf1 = RandomForestClassifier(random_state=1, n_estimators=50).fit(X_train_scaled, y_label)\n",
    "print(f'Training Score: {clf1.score(X_train_scaled, y_label)}')\n",
    "print(f'Testing Score: {clf1.score(X_test_scaled, y_test_label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random Forest scores: Unscaled data: Training Score: 1.0 Testing Score: 0.6707783921735432 Scaled data: Training Score: 1.0 Testing Score: 0.6714164185452999.\n",
    "\n",
    "Scaling the data had a very good effect on the outcome of the Log model. The scores for both the training and test sets increased, and the scores got closer together (0.13 difference reduced to 0.01), meaning the Log model is now likely to generalize well too. Scaling the data did not really have much effect on the Random Forest classifier. It still appears to be the case that the model is overfitting on the training data and thus performing poorly on the test data set. Overall, I believe the Log Model (with scaled data) is the better choice in this situation, since it seems to be less sensitive to noise in the data or any collinearity between features. I think we would see a drastic improvement in the Random Forest model if we implemented feature selection techniques, but that might be overkill for this use case. My predicition about the effect scaling would have on the models was not quite accurate. My first prediction, where I mention that random forests do not tend to generalize well, was closer to the truth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
